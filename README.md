# Information-Retrieved-HW

# HW1
In Homework 1, we leverage [Porter Stemming Alogrithm](https://tartarus.org/martin/PorterStemmer/) to stem the tokenizated words for a English news document. We also remove the stopword from [stopword list](https://github.com/a110605/Information-Retrieved-HW/blob/master/r03725019_hw1/stopwordlist.txt). Finally save the result to a txt file.  

<img src="https://github.com/a110605/Information-Retrieved-HW/blob/master/pictures/1.png" >

# HW2
In Homework 2, we have done the following two things.
  - we convert 1095 English news documents into tf-idf vectors and construct a dictionary based on the terms extracted from the given documents.
  - we write a function cosine (Docx, Docy) which loads the tf-idf vectors of documents x and y and returns their cosine similarity.

<img src="https://github.com/a110605/Information-Retrieved-HW/blob/master/pictures/2.png" >

# HW3
In homework 3, we implement [Naive Bayes Alogrithm](https://en.wikipedia.org/wiki/Naive_Bayes_classifier) on 1095 news documents to classify their classes.

The training set includes 13 classes (each class has 15 training documents). The remaining documents are for the testing.

Finally, we generate a output file to record our classification results.
HAC clustering:
 
 <img src="https://github.com/a110605/Information-Retrieved-HW/blob/master/pictures/3.png"  >


# HW4
In homework 4, we implement [HAC clustering alogrithm](https://en.wikipedia.org/wiki/Hierarchical_clustering) to separate 1095 documents for serveral clusters.

 <img src="https://github.com/a110605/Information-Retrieved-HW/blob/master/pictures/4.png" >
